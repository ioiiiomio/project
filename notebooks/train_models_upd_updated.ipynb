{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Training Updated Style Models\n", "\n", "This notebook trains four separate UNet-based autoencoder models for simple style imitation:\n", "\n", "- **blur**\n", "- **night_vis**\n", "- **poster** (posterization style)\n", "- **outline** (soft colored outline style)\n", "\n", "Each model learns a mapping:\n", "\\begin{align}\n", "\\text{input RGB image} \\;\\rightarrow\\; \\text{styled RGB image}\n", "\\end{align}\n", "\n", "The models are saved under `models/upd/autoencoder_{style}_upd.keras`."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Imports & configuration\n", "import os\n", "import cv2\n", "import numpy as np\n", "from sklearn.model_selection import train_test_split\n", "\n", "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Concatenate\n", "from tensorflow.keras.models import Model\n", "from tensorflow.keras.optimizers import Adam\n", "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n", "\n", "# -------- PATHS --------\n", "BASE_INPUT = \"/Users/amayakof/Desktop/2025_autumn/deep_learning/SIS/3/project/data/preprocessed/input\"\n", "STYLE_ROOT = \"/Users/amayakof/Desktop/2025_autumn/deep_learning/SIS/3/project/data/preprocessed/style\"\n", "MODEL_SAVE_DIR = \"/Users/amayakof/Desktop/2025_autumn/deep_learning/SIS/3/project/models/upd\"\n", "\n", "IMAGE_SIZE = (256, 256)\n", "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n", "\n", "# Style folders we expect under STYLE_ROOT\n", "STYLES = [\"blur\", \"night_vis\", \"poster\", \"outline\"]\n", "print(\"Planned styles:\", STYLES)"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Helper: load all RGB images from a folder and normalize to [0, 1]\n", "def load_images_from_folder(folder, size=IMAGE_SIZE):\n", "    imgs = []\n", "    files = sorted([\n", "        f for f in os.listdir(folder)\n", "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tiff\"))\n", "    ])\n", "\n", "    if not files:\n", "        print(f\"[WARN] No image files found in: {folder}\")\n", "\n", "    for fname in files:\n", "        path = os.path.join(folder, fname)\n", "        img = cv2.imread(path)\n", "        if img is None:\n", "            print(\"[WARN] Skipping unreadable:\", path)\n", "            continue\n", "\n", "        # BGR -> RGB for consistency\n", "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n", "        img = cv2.resize(img, size)\n", "        img = img.astype(\"float32\") / 255.0\n", "        imgs.append(img)\n", "\n", "    return np.array(imgs)"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# UNet-style autoencoder architecture for 256x256x3 images\n", "def build_unet_autoencoder(input_shape=(256, 256, 3)):\n", "    inp = Input(shape=input_shape)\n", "\n", "    # Encoder\n", "    c1 = Conv2D(32, (3,3), activation=\"relu\", padding=\"same\")(inp)\n", "    p1 = MaxPooling2D((2,2))(c1)              # 256 -> 128\n", "\n", "    c2 = Conv2D(64, (3,3), activation=\"relu\", padding=\"same\")(p1)\n", "    p2 = MaxPooling2D((2,2))(c2)              # 128 -> 64\n", "\n", "    c3 = Conv2D(128, (3,3), activation=\"relu\", padding=\"same\")(p2)\n", "    p3 = MaxPooling2D((2,2))(c3)              # 64 -> 32\n", "\n", "    # Bottleneck\n", "    bn = Conv2D(256, (3,3), activation=\"relu\", padding=\"same\")(p3)\n", "\n", "    # Decoder\n", "    u3 = UpSampling2D((2,2))(bn)              # 32 -> 64\n", "    u3 = Concatenate()([u3, c3])\n", "    d3 = Conv2D(128, (3,3), activation=\"relu\", padding=\"same\")(u3)\n", "\n", "    u2 = UpSampling2D((2,2))(d3)              # 64 -> 128\n", "    u2 = Concatenate()([u2, c2])\n", "    d2 = Conv2D(64, (3,3), activation=\"relu\", padding=\"same\")(u2)\n", "\n", "    u1 = UpSampling2D((2,2))(d2)              # 128 -> 256\n", "    u1 = Concatenate()([u1, c1])\n", "    d1 = Conv2D(32, (3,3), activation=\"relu\", padding=\"same\")(u1)\n", "\n", "    out = Conv2D(3, (3,3), activation=\"sigmoid\", padding=\"same\")(d1)\n", "\n", "    model = Model(inp, out)\n", "    model.compile(optimizer=Adam(1e-4), loss=\"mae\")\n", "    return model\n", "\n", "print(\"UNet autoencoder ready.\")"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Load input images once (these are the 'content' images)\n", "print(\"Loading INPUT images from:\", BASE_INPUT)\n", "input_images = load_images_from_folder(BASE_INPUT)\n", "print(\"Input images shape:\", input_images.shape)\n", "\n", "if input_images.size == 0:\n", "    raise RuntimeError(\"No input images loaded. Check BASE_INPUT path and files.\")"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Helper function: train one style-specific model\n", "def train_style_model(style_name,\n", "                      base_inputs,\n", "                      epochs=40,\n", "                      batch_size=2,\n", "                      val_split=0.15):\n", "    \"\"\"Train a UNet autoencoder to map input -> style images for a given style_name.\"\"\"\n", "\n", "    style_folder = os.path.join(STYLE_ROOT, style_name)\n", "    print(\"\\n====================================\")\n", "    print(f\" TRAINING STYLE: {style_name.upper()}\")\n", "    print(\" Style folder:\", style_folder)\n", "    print(\"====================================\")\n", "\n", "    if not os.path.isdir(style_folder):\n", "        print(f\"[ERROR] Style folder does not exist: {style_folder}\")\n", "        return\n", "\n", "    style_images = load_images_from_folder(style_folder)\n", "    print(\"Style images shape:\", style_images.shape)\n", "\n", "    if style_images.size == 0:\n", "        print(\"[ERROR] No style images found, skipping style:\", style_name)\n", "        return\n", "\n", "    if len(style_images) != len(base_inputs):\n", "        print(\"[ERROR] COUNT MISMATCH for style: \", style_name)\n", "        print(\"        #input_images =\", len(base_inputs))\n", "        print(\"        #style_images =\", len(style_images))\n", "        print(\"        Ensure 1-1 pairing between input and style images.\")\n", "        return\n", "\n", "    X_train, X_val, y_train, y_val = train_test_split(\n", "        base_inputs, style_images,\n", "        test_size=val_split,\n", "        random_state=42\n", "    )\n", "\n", "    model = build_unet_autoencoder()\n", "\n", "    save_path = os.path.join(MODEL_SAVE_DIR, f\"autoencoder_{style_name}_upd.keras\")\n", "    callbacks = [\n", "        EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True),\n", "        ModelCheckpoint(save_path, monitor=\"val_loss\", save_best_only=True)\n", "    ]\n", "\n", "    history = model.fit(\n", "        X_train, y_train,\n", "        validation_data=(X_val, y_val),\n", "        epochs=epochs,\n", "        batch_size=batch_size,\n", "        callbacks=callbacks,\n", "        verbose=1\n", "    )\n", "\n", "    print(f\"\\nSaved updated model for style '{style_name}' to:\\n  {save_path}\\n\")\n", "    return history\n", "\n", "print(\"Training helper ready.\")"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Train BLUR model\n", "hist_blur = train_style_model(\"blur\", input_images, epochs=40, batch_size=2)"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Train NIGHT_VIS model\n", "hist_night = train_style_model(\"night_vis\", input_images, epochs=40, batch_size=2)"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Train POSTER model\n", "hist_poster = train_style_model(\"poster\", input_images, epochs=40, batch_size=2)"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Train OUTLINE model\n", "hist_outline = train_style_model(\"outline\", input_images, epochs=40, batch_size=2)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.11"}}, "nbformat": 4, "nbformat_minor": 5}