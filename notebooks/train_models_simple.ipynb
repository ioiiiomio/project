{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "454cf3e7",
   "metadata": {},
   "source": [
    "# Shared code block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cbe227a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "IMAGE_SIZE = (256, 256)\n",
    "\n",
    "def load_images_from_folder(folder, size=IMAGE_SIZE):\n",
    "    imgs = []\n",
    "    files = sorted([\n",
    "        f for f in os.listdir(folder)\n",
    "        if f.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tiff\"))\n",
    "    ])\n",
    "\n",
    "    for fname in files:\n",
    "        path = os.path.join(folder, fname)\n",
    "        img = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "        if img is None:\n",
    "            print(\"Skipping unreadable:\", fname)\n",
    "            continue\n",
    "\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, size)\n",
    "        img = img.astype(\"float32\") / 255.0\n",
    "        imgs.append(img)\n",
    "\n",
    "    return np.array(imgs)\n",
    "\n",
    "# ---- Custom clip layer replacing Lambda ----\n",
    "class Clip01(tf.keras.layers.Layer):\n",
    "    def call(self, x):\n",
    "        return tf.clip_by_value(x, 0.0, 1.0)\n",
    "\n",
    "\n",
    "def build_autoencoder(input_shape=(256, 256, 3)):\n",
    "    inp = Input(shape=input_shape)\n",
    "\n",
    "    # ----- Encoder -----\n",
    "    x = Conv2D(32, (3,3), activation=\"relu\", padding=\"same\")(inp)\n",
    "    x = MaxPooling2D((2,2), padding=\"same\")(x)\n",
    "\n",
    "    x = Conv2D(64, (3,3), activation=\"relu\", padding=\"same\")(x)\n",
    "    x = MaxPooling2D((2,2), padding=\"same\")(x)\n",
    "\n",
    "    # ----- Bottleneck -----\n",
    "    x = Conv2D(128, (3,3), activation=\"relu\", padding=\"same\")(x)\n",
    "\n",
    "    # ----- Decoder -----\n",
    "    x = UpSampling2D((2,2))(x)\n",
    "    x = Conv2D(64, (3,3), activation=\"relu\", padding=\"same\")(x)\n",
    "\n",
    "    x = UpSampling2D((2,2))(x)\n",
    "    x = Conv2D(32, (3,3), activation=\"relu\", padding=\"same\")(x)\n",
    "\n",
    "    # Raw output before skip\n",
    "    out_raw = Conv2D(3, (3,3), activation=\"sigmoid\", padding=\"same\")(x)\n",
    "    out = out_raw\n",
    "\n",
    "    # Clipping using custom layer (NO lambda!)\n",
    "    out = Clip01()\n",
    "\n",
    "    model = Model(inp, out)\n",
    "    model.compile(optimizer=Adam(1e-3), loss=\"mse\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5b1c969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading input images...\n",
      "(13, 256, 256, 3)\n",
      "Loading BLUR style images...\n",
      "(0,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Mismatch in BLUR training set!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(blur_images\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(input_images) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(blur_images):\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMismatch in BLUR training set!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m model \u001b[38;5;241m=\u001b[39m build_autoencoder()\n\u001b[1;32m     20\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m     21\u001b[0m     input_images,\n\u001b[1;32m     22\u001b[0m     blur_images,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     27\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Mismatch in BLUR training set!"
     ]
    }
   ],
   "source": [
    "# blur\n",
    "BASE_INPUT = \"/Users/amayakof/Desktop/2025_autumn/deep_learning/SIS/3/project/data/preprocessed/input\"\n",
    "STYLE_FOLDER = \"/Users/amayakof/Desktop/2025_autumn/deep_learning/SIS/3/project/data/preprocessed/style/blur\"\n",
    "MODEL_SAVE_DIR = \"/Users/amayakof/Desktop/2025_autumn/deep_learning/SIS/3/project/models\"\n",
    "\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Loading input images...\")\n",
    "input_images = load_images_from_folder(BASE_INPUT)\n",
    "print(input_images.shape)\n",
    "\n",
    "print(\"Loading BLUR style images...\")\n",
    "blur_images = load_images_from_folder(STYLE_FOLDER)\n",
    "print(blur_images.shape)\n",
    "\n",
    "if len(input_images) != len(blur_images):\n",
    "    raise ValueError(\"Mismatch in BLUR training set!\")\n",
    "\n",
    "model = build_autoencoder()\n",
    "model.fit(\n",
    "    input_images,\n",
    "    blur_images,\n",
    "    epochs=50,\n",
    "    batch_size=4,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model.save(f\"{MODEL_SAVE_DIR}/autoencoder_blur.keras\")\n",
    "print(\"Saved BLUR model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "377fba91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading input images...\n",
      "Loading Night Vision style images...\n",
      "Epoch 1/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 788ms/step - loss: 0.6896WARNING:tensorflow:5 out of the last 207 calls to <function TensorFlowTrainer._make_function.<locals>.multi_step_on_iterator at 0x362ea8a40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1s/step - loss: 0.6861 - val_loss: 0.6638\n",
      "Epoch 2/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step - loss: 0.6081 - val_loss: 0.4989\n",
      "Epoch 3/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 916ms/step - loss: 0.4754 - val_loss: 0.4036\n",
      "Epoch 4/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 811ms/step - loss: 0.4229 - val_loss: 0.3958\n",
      "Epoch 5/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step - loss: 0.4177 - val_loss: 0.3956\n",
      "Epoch 6/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1s/step - loss: 0.4175 - val_loss: 0.3956\n",
      "Epoch 7/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 751ms/step - loss: 0.4175 - val_loss: 0.3956\n",
      "Epoch 8/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 777ms/step - loss: 0.4175 - val_loss: 0.3956\n",
      "Epoch 9/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 743ms/step - loss: 0.4175 - val_loss: 0.3956\n",
      "Epoch 10/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 730ms/step - loss: 0.4175 - val_loss: 0.3956\n",
      "Saved  Night Vision model.\n"
     ]
    }
   ],
   "source": [
    "# night_vis\n",
    "\n",
    "import os\n",
    "\n",
    "BASE_INPUT = \"/Users/amayakof/Desktop/2025_autumn/deep_learning/SIS/3/project/data/preprocessed/input\"\n",
    "STYLE_FOLDER = \"/Users/amayakof/Desktop/2025_autumn/deep_learning/SIS/3/project/data/preprocessed/style/night_vis\"\n",
    "MODEL_SAVE_DIR = \"/Users/amayakof/Desktop/2025_autumn/deep_learning/SIS/3/project/models\"\n",
    "\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Loading input images...\")\n",
    "input_images = load_images_from_folder(BASE_INPUT)\n",
    "\n",
    "print(\"Loading Night Vision style images...\")\n",
    "night_vis_images = load_images_from_folder(STYLE_FOLDER)\n",
    "\n",
    "if len(input_images) != len(night_vis_images):\n",
    "    raise ValueError(\"Mismatch in Night Vision training set!\")\n",
    "\n",
    "model = build_autoencoder()\n",
    "model.fit(\n",
    "    input_images,\n",
    "    night_vis_images,\n",
    "    epochs=10,\n",
    "    batch_size=4,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model.save(f\"{MODEL_SAVE_DIR}/autoencoder_night_vis.keras\")\n",
    "print(\"Saved  Night Vision model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a816857",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# outline\n",
    "import os\n",
    "\n",
    "BASE_INPUT = \"/Users/amayakof/Desktop/2025_autumn/deep_learning/SIS/3/project/data/preprocessed/input\"\n",
    "STYLE_FOLDER = \"/Users/amayakof/Desktop/2025_autumn/deep_learning/SIS/3/project/data/preprocessed/style/outline\"\n",
    "MODEL_SAVE_DIR = \"/Users/amayakof/Desktop/2025_autumn/deep_learning/SIS/3/project/models\"\n",
    "\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Loading input images...\")\n",
    "input_images = load_images_from_folder(BASE_INPUT)\n",
    "\n",
    "print(\"Loading OUTLINE style images...\")\n",
    "outline_images = load_images_from_folder(STYLE_FOLDER)\n",
    "\n",
    "if len(input_images) != len(outline_images):\n",
    "    raise ValueError(\"Mismatch in OUTLINE training set!\")\n",
    "\n",
    "model = build_autoencoder()\n",
    "model.fit(\n",
    "    input_images,\n",
    "    outline_images,\n",
    "    epochs=50,\n",
    "    batch_size=4,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model.save(f\"{MODEL_SAVE_DIR}/autoencoder_outline.keras\")\n",
    "print(\"Saved OUTLINE model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d844b238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# poster style\n",
    "import os\n",
    "\n",
    "BASE_INPUT = \"/Users/amayakof/Desktop/2025_autumn/deep_learning/SIS/3/project/data/preprocessed/input\"\n",
    "STYLE_FOLDER = \"/Users/amayakof/Desktop/2025_autumn/deep_learning/SIS/3/project/data/preprocessed/style/poster\"\n",
    "MODEL_SAVE_DIR = \"/Users/amayakof/Desktop/2025_autumn/deep_learning/SIS/3/project/models\"\n",
    "\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Loading input images...\")\n",
    "input_images = load_images_from_folder(BASE_INPUT)\n",
    "\n",
    "print(\"Loading POSTER style images...\")\n",
    "poster_images = load_images_from_folder(STYLE_FOLDER)\n",
    "\n",
    "if len(input_images) != len(poster_images):\n",
    "    raise ValueError(\"Mismatch in POSTER training set!\")\n",
    "\n",
    "model = build_autoencoder()\n",
    "model.fit(\n",
    "    input_images,\n",
    "    poster_images,\n",
    "    epochs=50,\n",
    "    batch_size=4,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model.save(f\"{MODEL_SAVE_DIR}/autoencoder_poster.keras\")\n",
    "print(\"Saved POSTER model.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
